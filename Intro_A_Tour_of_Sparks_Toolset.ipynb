{"cells":[{"cell_type":"markdown","source":["### Running Production Applications\n* turn your interactive exploration into production applications with spark-submit\n* spark-submit lets you send your application code to a cluster and launch it to execute there.\n* You can do this with all of Sparks support cluster managers \n#####Structured APIs - Datasets, DataFrames, SQL\n#####Low-level APIs - RDDs, Distributed Variables"],"metadata":{}},{"cell_type":"markdown","source":["Running sample Python version application on local machine, by running an app in the directory where you downloaded the spark\n```./bin/spark-submit --master local ./examples/src/main/python/pi.py 10```\nBy changing the master argument, you can submit the same app to Cluster manager, YARN or MESOS."],"metadata":{}},{"cell_type":"markdown","source":["####Datasets:Type-Safe Structured APIs\n* Dataset API is not available in Python and R - as they are dynamically typed\n* DataFrames area adistributed collection of objects of type Row, that can hold various types of tabular data\n* Dataset API gives users the ability to assign a Java/Scala class to the records within a DataFrame and manipulate it as a collection of typed objects, similar to a Java ArrayList or Scala Seq."],"metadata":{}},{"cell_type":"code","source":["staticDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/FileStore/tables/2010_12_01-ec65d.csv\")\n#.csv(\"/FileStore/tables/2015_summary-ebaee.csv\")\n\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = staticDataFrame.schema\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import window, column, desc, col\nstaticDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\\\n  .show(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   13408.0|[2010-12-01 00:00...|1024.6800000000003|\n   17460.0|[2010-12-01 00:00...|              19.9|\n   15235.0|[2010-12-01 00:00...|              79.5|\n   17905.0|[2010-12-01 00:00...|201.74999999999997|\n   12583.0|[2010-12-01 00:00...|            855.86|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["streamingDataFrame = spark.readStream\\\n    .schema(staticSchema)\\\n    .option(\"maxFilesPerTrigger\", 1)\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .load(\"/FileStore/tables/*.csv\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["purchaseByCustomerPerHour = streamingDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["purchaseByCustomerPerHour.writeStream\\\n    .format(\"memory\")\\\n    .queryName(\"customer_purchases\")\\\n    .outputMode(\"complete\")\\\n    .start()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">20</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.streaming.StreamingQuery at 0x7fa856de0a50&gt;\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["spark.sql(\"\"\"\n  SELECT *\n  FROM customer_purchases\n  ORDER BY `sum(total_cost)` DESC\n  \"\"\")\\\n  .show(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+---------------+\nCustomerId|window|sum(total_cost)|\n+----------+------+---------------+\n+----------+------+---------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["###Machine Learning and Advanced Analytics\n* MLlib - allows for preprocessing, munging, training of models, and making predictions at scale on data\n* Models trained in MLlib can be used to make predictions in Structured Streaming as well.\n* MLlib algorithms require the data is represented as numerical values."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col\npreppedDataFrame = staticDataFrame\\\n  .na.fill(0)\\\n  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n  .coalesce(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["trainDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate < '2011-07-01'\")\ntestDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate >= '2011-07-01'\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer()\\\n  .setInputCol(\"day_of_week\")\\\n  .setOutputCol(\"day_of_week_index\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["* StringIndexer - converts text to numerical values eg.Staurday to 6.\n* OneHotEncoder - converts each of those numbers as their own column - to boolean values.\n* All Machine learning algorithms in spark take input as a Vector type, which must be a set of numerical values.(VectorAssembler)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder()\\\n  .setInputCol(\"day_of_week_index\")\\\n  .setOutputCol(\"day_of_week_encoded\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler()\\\n  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n  .setOutputCol(\"features\")\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["# set this process into a pipeline so that any future data we need to transform can go through the exact same process.\n# preparing for training is a two step process 1. Fit 2. Transform\n# Fit bcz StringIndexer needs to know how many unique values there are to be indexed.\nfrom pyspark.ml import Pipeline\n\ntransformationPipeline = Pipeline()\\\n  .setStages([indexer, encoder, vectorAssembler])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["fittedPipeline = transformationPipeline.fit(trainDataFrame)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["transformedTraining = fittedPipeline.transform(trainDataFrame)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3920840169979258&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>transformedTraining <span class=\"ansiyellow\">=</span> fittedPipeline<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>trainDataFrame<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansicyan\">transform</span><span class=\"ansiblue\">(self, dataset, params)</span>\n<span class=\"ansigreen\">    171</span>                 <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>copy<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    172</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 173</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    174</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    175</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Params must be a param map but got %s.&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/pipeline.py</span> in <span class=\"ansicyan\">_transform</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    260</span>     <span class=\"ansigreen\">def</span> _transform<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    261</span>         <span class=\"ansigreen\">for</span> t <span class=\"ansigreen\">in</span> self<span class=\"ansiyellow\">.</span>stages<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 262</span><span class=\"ansiyellow\">             </span>dataset <span class=\"ansiyellow\">=</span> t<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    263</span>         <span class=\"ansigreen\">return</span> dataset<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    264</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansicyan\">transform</span><span class=\"ansiblue\">(self, dataset, params)</span>\n<span class=\"ansigreen\">    171</span>                 <span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>copy<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    172</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 173</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    174</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    175</span>             <span class=\"ansigreen\">raise</span> ValueError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;Params must be a param map but got %s.&quot;</span> <span class=\"ansiyellow\">%</span> type<span class=\"ansiyellow\">(</span>params<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/ml/wrapper.py</span> in <span class=\"ansicyan\">_transform</span><span class=\"ansiblue\">(self, dataset)</span>\n<span class=\"ansigreen\">    303</span>     <span class=\"ansigreen\">def</span> _transform<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    304</span>         self<span class=\"ansiyellow\">.</span>_transfer_params_to_java<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 305</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_java_obj<span class=\"ansiyellow\">.</span>transform<span class=\"ansiyellow\">(</span>dataset<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> dataset<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    306</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    307</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     77</span>                 <span class=\"ansigreen\">raise</span> QueryExecutionException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     78</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;java.lang.IllegalArgumentException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 79</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> IllegalArgumentException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     80</span>             <span class=\"ansigreen\">raise</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     81</span>     <span class=\"ansigreen\">return</span> deco<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">IllegalArgumentException</span>: u&apos;requirement failed: The input column day_of_week_index should have at least two distinct values.&apos;</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# In Spark, training ML models is a two-phase process. First we initialize an untrained model(Algorithm - KMeans), and then train it (AlgotithmModel-kmModel)\nfrom pyspark.ml.clustering import KMeans\nkmeans = KMeans()\\\n  .setK(20)\\\n  .setSeed(1L)\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["kmModel = kmeans.fit(transformedTraining)\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["transformedTest = fittedPipeline.transform(testDataFrame)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["###Lower-Level APIs\n* Spark include a number of lower-level primitives to allow for arbitrary Java and Python object manipulation via RDDs. Virtually everything in Spark is built on top of RDDs.\n* DataFrames are compile down to these lower-level tools for convenient and extremely edfficient distributed execution.\n* RDDs are lower level than DataFrames bcz they reveal physical execution characteristics (like partitions to end users)"],"metadata":{}},{"cell_type":"code","source":["# One thing you might use RDDs for is to parallelize raw data that you have stored in memory on the driver machine.\nfrom pyspark.sql import Row\n\nspark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25}],"metadata":{"name":"A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset","notebookId":3920840169979244},"nbformat":4,"nbformat_minor":0}
