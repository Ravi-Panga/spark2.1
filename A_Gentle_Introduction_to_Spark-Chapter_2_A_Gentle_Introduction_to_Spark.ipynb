{"cells":[{"cell_type":"code","source":["# SparkSession - you control your spark app through driver process called SparkSession, which is available as Spark variable\nspark"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">1</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7f5bdea4f3d0&gt;\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":["# One Column Containing 1000 rows with values from 0 to 99\nmyRange = spark.range(1000).toDF(\"number\")\n# Spark DataFrame\n# This range of numbers exists on a distributed collection. When run on a cluster,each part of this range of numbers exists on a different executor"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## DataFrames\nDataFrame is the most common **Structured API**, represents a table of data with rows and columns.\n**Schema** is the list that defines the columns and the types of within those columns. Spark has several core abstractions (distriburted collections of data) like Datasets, DataFrames, SQL tables and RDD's.\n### Partitions\n* to allow every executor to perform work in parallel, spark breaks up the data into partitions (a collection of rows that sit on one physical machine in your cluster)\n* if you have one partition and thousand executors or vice versa - spark will have parallelism of only one because you have only one computation resource.\n* with DataFrames you don't manipulate manually\n## Transformations\nIn spark, core data structures are immutable, you can modify them - called Transformations\n* Narrow transformations - contributes to only on output partition (one to one)\n* Wide transdformations - dependencies contributing to many output partitions (one to many)\n### Lazy Evaluation\nSpark will wait until the very last moment to execute the graph of computation instructions. Instead of modifying the data immediately, saprk build up a plan of transformations til the last minute. This makes spark optimize the entire data flow from end to end."],"metadata":{}},{"cell_type":"code","source":["# transformation \ndivisBy2 = myRange.where(\"number % 2 = 0\")\n# scala val divisBy2 = myRange.where(\"number % 2 = 0\")\n# Note: these return no output bcz we specified only an abstract transformation, spark will not act on transformation until we call an action"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["<span style=\"color:brown\">some *brown* text</span>.\n##Action\ntransformations allows us to buildup our logical transformation plan. To trigger the computation, we run an action. example: divisBy2**.count()**\n* to view in the console\n* to collect data to native objects in the respective language\n* to write to output data sources\n## SparkUI\nyou can monitor the progress of a job through the Spark web UI, available on port 4040 of the driver node.(if running on local mode http://localhost:4040)\nSparkUI - displays the state of your spark jobs, its environment and cluster state. useful for tuning and debugging.\n**\"Spark job\"** represents the transformations triggered by an individual action."],"metadata":{}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>500\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["# An End-to-End Example \n**schema inference**- spark takes the best guess at what the schema of our DataFrame should be.\nTo get schema information, Spark readsd in a little bit of the data and then attempts to parse the types in those rows according to the types available in spark. Recommended to strictly specify the schema in production scenarios."],"metadata":{}},{"cell_type":"code","source":["flightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/FileStore/tables/2015_summary-ebaee.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# converted into local array or list of rows - Array(row(...),Row(...))\nflightData2015.take(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Romania&apos;, count=15),\n Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Croatia&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Ireland&apos;, count=344)]\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["sort doesn't modify the DataFrame, instead that returns a new DataFrame by transforming the previous DataFrame\n**CSV ->(read-narrow) DF -> (sort-wide) DF -> (take(3)) Array(...)\nReading -> Sorting -> Collecting a DF.**\nsort is wide transformation, bcz rows will need to be compared with one another."],"metadata":{}},{"cell_type":"code","source":["# explain on any DataFrame object, to see how spark will execute this query\nflightData2015.sort(\"count\").explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Sort [count#39 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#39 ASC NULLS FIRST, 200)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#37,ORIGIN_COUNTRY_NAME#38,count#39] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# By default, when we perform shuffle operation, spark outputs 200 shuffle partitions \nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\nflightData2015.sort(\"count\").take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Singapore&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1)]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# By default, when we perform shuffle operation, spark outputs 200 shuffle partitions \nspark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\nflightData2015.sort(\"count\").take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Singapore&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1)]\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["**the logical plan that we build up defines a lineage for the DF, so that at any given point of time Spark know how to recompute any partition by performing all of the operations it had before on the same data (Functional programming is at the spark's core)**"],"metadata":{}},{"cell_type":"markdown","source":["###DataFrames and SQL\nYou can express your business logic in SQL or DataFrames (either in R,Scala,Python, or Java) and Spark will compile that logic down to an Underlying plan(that you can see in the explain plan)before actually executing the code.\n**No performance difference between writing SQL queries or writing DataFrame code, they both \"compile\" to the same underlying plan that we specify in the DF code**"],"metadata":{}},{"cell_type":"code","source":["#can make any df into a table or view with one simple method call\nflightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["# the SQL query against a df returns another df - Powerful, as it is possible for you to  specify the transformations in the manner you want \n# without compromising on efficency. Below the two physical plans are exactly the same\nsqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\nsqlWay.explain()\ndataFrameWay.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[finalmerge_count(merge count#70L) AS count(1)#58L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#37, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[partial_count(1) AS count#70L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#37] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[finalmerge_count(merge count#72L) AS count(1)#65L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#37, 200)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[partial_count(1) AS count#72L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#37] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">12</span><span class=\"ansired\">]: </span>[Row(max(count)=370002)]\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nTakeOrderedAndProject(limit=5, orderBy=[destination_total#129L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#37,destination_total#129L])\n+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[finalmerge_sum(merge sum#135L) AS sum(cast(count#39 as bigint))#125L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#37, 200)\n      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#37], functions=[partial_sum(cast(count#39 as bigint)) AS sum#135L])\n         +- *(1) FileScan csv [DEST_COUNTRY_NAME#37,count#39] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary-ebaee.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["**Spark lazily executes a DAG of transformations in order to optimize the execution plan on DF's. **"],"metadata":{}}],"metadata":{"name":"A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark","notebookId":2688922167666563},"nbformat":4,"nbformat_minor":0}
